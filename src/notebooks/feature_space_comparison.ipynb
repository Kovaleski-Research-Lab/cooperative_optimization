{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38261634-b559-46f2-9100-8947e2e078f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import yaml\n",
    "import sys\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "sys.path.append('../')\n",
    "from datamodule.datamodule import select_data\n",
    "from models.models import Classifier, CooperativeOpticalModelRemote\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "from sklearn import datasets, decomposition\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0056d0-e8ba-4e20-a8af-ed63e758715e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(plt.style.available)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05435808-eebf-4817-b7fc-82ecbaf5695b",
   "metadata": {},
   "source": [
    "# Load in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85036d9b-b49c-4d0f-901a-f4bd43cae27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = yaml.load(open('../../config.yaml', 'r'), Loader=yaml.FullLoader)\n",
    "config['paths']['path_root'] = '../../'\n",
    "config['paths']['path_data'] = 'data/baseline'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588892ec-96e1-4e3a-a0cd-b4f81f90d275",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_filenames = [os.path.join(config['paths']['path_root'], config['paths']['path_data'], i) for i in os.listdir(os.path.join(config['paths']['path_root'], config['paths']['path_data']))]\n",
    "baseline_filenames.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d613ad09-31ef-4737-bba0-b7bb99935cfa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "baseline_bench_images = [torch.load(f, weights_only=True)['bench_image'].squeeze().numpy() for f in tqdm(baseline_filenames)]\n",
    "baseline_sim_images = [torch.load(f, weights_only=True)['sim_output'].squeeze().numpy() for f in tqdm(baseline_filenames)]\n",
    "baseline_ideal_images = [torch.load(f, weights_only=True)['resampled_sample'].squeeze().numpy() for f in tqdm(baseline_filenames)]\n",
    "\n",
    "baseline_targets = [torch.argmax(torch.load(f, weights_only=True)['target']).numpy() for f in tqdm(baseline_filenames)]\n",
    "baseline_targets = np.asarray(baseline_targets).squeeze()\n",
    "baseline_unique_targets = np.unique(baseline_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c588c37a-fe73-4d35-ac40-04f3c9569f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config['paths']['path_data'] = 'data/post_training'\n",
    "pt_filenames = [os.path.join(config['paths']['path_root'], config['paths']['path_data'], i) for i in os.listdir(os.path.join(config['paths']['path_root'], config['paths']['path_data']))]\n",
    "pt_filenames.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a4e6d0-9e84-49da-865e-69059f46f00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_bench_images = [torch.load(f, weights_only=True)['bench_image'].squeeze().detach().numpy() for f in tqdm(pt_filenames)]\n",
    "pt_sim_images = [torch.load(f, weights_only=True)['sim_output'].squeeze().detach().numpy() for f in tqdm(pt_filenames)]\n",
    "pt_ideal_images = [torch.load(f, weights_only=True)['resampled_sample'].squeeze().detach().numpy() for f in tqdm(pt_filenames)]\n",
    "\n",
    "pt_targets = [torch.argmax(torch.load(f, weights_only=True)['target']).numpy() for f in tqdm(pt_filenames)]\n",
    "pt_targets = np.asarray(pt_targets).squeeze()\n",
    "pt_unique_targets = np.unique(pt_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e3e966-a33f-4cd6-b3b9-db4c055bb197",
   "metadata": {},
   "source": [
    "# Load in the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b4ab5d-7776-4173-82ba-3158e423daf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = '../../results/classifier_baseline_bench_resampled_sample/version_0/checkpoints/last.ckpt'\n",
    "classifier = Classifier.load_from_checkpoint(checkpoint_path).double().cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b24c0d8-f47e-420e-ac58-e5844995328c",
   "metadata": {},
   "source": [
    "# Populate the feature representations for the different images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b329728-2802-4d54-a94e-36f7b65c501b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_vectors(classifier, images):\n",
    "    feature_vectors = []\n",
    "    for image in tqdm(images):\n",
    "        image = torch.from_numpy(image).squeeze().unsqueeze(0).unsqueeze(0)\n",
    "        image = torch.cat([image, image, image], dim=1).double()\n",
    "        feature_vectors.append(classifier.feature_extractor(image))\n",
    "    return feature_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddb0a05-b89f-47ee-a12b-5c7e0530cb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    baseline_bench_feature_embeddings = create_feature_vectors(classifier, baseline_bench_images)\n",
    "    baseline_sim_feature_embeddings = create_feature_vectors(classifier, baseline_sim_images)\n",
    "    baseline_ideal_feature_embeddings = create_feature_vectors(classifier, baseline_ideal_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0afd8ce-a8c5-4c3a-a438-5e71f706242c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    pt_bench_feature_embeddings = create_feature_vectors(classifier, pt_bench_images)\n",
    "    pt_sim_feature_embeddings = create_feature_vectors(classifier, pt_sim_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea45ad1-799c-4a3e-a822-e12d62438fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_results = '../../results/feature_embeddings/'\n",
    "os.makedirs(path_results, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c68abc-249b-4c4d-82c1-b4e7c1ad1ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(baseline_bench_feature_embeddings, os.path.join(path_results, 'baseline_bench_feature_embeddings.pt'))\n",
    "torch.save(baseline_sim_feature_embeddings, os.path.join(path_results, 'baseline_sim_feature_embeddings.pt'))\n",
    "torch.save(baseline_ideal_feature_embeddings, os.path.join(path_results, 'baseline_ideal_feature_embeddings.pt'))\n",
    "torch.save(pt_bench_feature_embeddings, os.path.join(path_results, 'pt_bench_feature_embeddings.pt'))\n",
    "torch.save(pt_sim_feature_embeddings, os.path.join(path_results, 'pt_sim_feature_embeddings.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26de1fd0-120f-4f3e-8d71-e0c11e1bbc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_bench_feature_embeddings = torch.load(os.path.join(path_results, 'baseline_bench_feature_embeddings.pt'), weights_only=True)\n",
    "baseline_sim_feature_embeddings = torch.load(os.path.join(path_results, 'baseline_sim_feature_embeddings.pt'), weights_only=True)\n",
    "baseline_ideal_feature_embeddings = torch.load(os.path.join(path_results, 'baseline_ideal_feature_embeddings.pt'), weights_only=True)\n",
    "pt_bench_feature_embeddings = torch.load(os.path.join(path_results, 'pt_bench_feature_embeddings.pt'), weights_only=True)\n",
    "pt_sim_feature_embeddings = torch.load(os.path.join(path_results, 'pt_sim_feature_embeddings.pt'), weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29de988d-ec40-4e05-b475-9edec5b375e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_bench_feature_embeddings = np.asarray([np.asarray(i.squeeze().detach().cpu()) for i in baseline_bench_feature_embeddings])\n",
    "baseline_sim_feature_embeddings = np.asarray([np.asarray(i.squeeze().detach().cpu()) for i in baseline_sim_feature_embeddings])\n",
    "baseline_ideal_feature_embeddings = np.asarray([np.asarray(i.squeeze().detach().cpu()) for i in baseline_ideal_feature_embeddings])\n",
    "pt_bench_feature_embeddings = np.asarray([np.asarray(i.squeeze().detach().cpu()) for i in pt_bench_feature_embeddings])\n",
    "pt_sim_feature_embeddings = np.asarray([np.asarray(i.squeeze().detach().cpu()) for i in pt_sim_feature_embeddings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4c56b2-bf96-4627-9692-d9ee66a59080",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_bench_feature_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb3951b-e6e8-45a8-a349-7a4e59e8791a",
   "metadata": {},
   "source": [
    "# Colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162dd303-80d7-42aa-82dc-8e6a15126003",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['#E8ECFB', '#D9CCE3', '#D1BBD7', '#CAACCB', '#BA8DB4', \n",
    "          '#AE76A3', '#AA6F9E', '#994F88', '#882E72', '#1965B0', \n",
    "          '#437DBF', '#5289C7', '#6195CF', '#7BAFDE', '#4EB265', \n",
    "          '#90C987', '#CAE0AB', '#F7F056', '#F7CB45', '#F6C141', \n",
    "          '#F4A736', '#F1932D', '#EE8026', '#E8601C', '#E65518', \n",
    "          '#DC050C', '#A5170E', '#72190E', '#42150A']\n",
    "\n",
    "colors2 = [ '#a6cee3',\n",
    "            '#1f78b4',\n",
    "            '#b2df8a',\n",
    "            '#33a02c',\n",
    "            '#fb9a99',\n",
    "            '#e31a1c',\n",
    "            '#fdbf6f',\n",
    "            '#ff7f00',\n",
    "            '#cab2d6',\n",
    "            '#6a3d9a']\n",
    "\n",
    "color_indices = [9,10,14,15,17,18,21,24,26,28]\n",
    "color2_indices = [0,1,2,3,4,5,6,7,8,9]\n",
    "len(color_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171b5078-e953-4213-ab07-edb089b52cb5",
   "metadata": {},
   "source": [
    "# PCA comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a291ece-853f-4053-8a2f-d2fb9a8041e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "pca.fit(baseline_ideal_feature_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d1f03c-5ae9-412e-891e-6595502d6f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_bench_pca = pca.transform(baseline_bench_feature_embeddings)\n",
    "baseline_sim_pca = pca.transform(baseline_sim_feature_embeddings)\n",
    "baseline_ideal_pca = pca.transform(baseline_ideal_feature_embeddings)\n",
    "pt_bench_pca = pca.transform(pt_bench_feature_embeddings)\n",
    "pt_sim_pca = pca.transform(pt_sim_feature_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4749fb0b-38f2-4a25-aece-9120b50c192a",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_targets = [torch.argmax(torch.load(f, weights_only=True)['target']).numpy() for f in tqdm(baseline_filenames)]\n",
    "baseline_targets = np.asarray(baseline_targets).squeeze()\n",
    "baseline_unique_targets = np.unique(baseline_targets)\n",
    "pt_targets = [torch.argmax(torch.load(f, weights_only=True)['target']).numpy() for f in tqdm(pt_filenames)]\n",
    "pt_targets = np.asarray(pt_targets).squeeze()\n",
    "pt_unique_targets = np.unique(pt_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8e2746-623c-4d29-bc80-3a305c602065",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(2,3, figsize=(15,10))\n",
    "\n",
    "for target in baseline_unique_targets:\n",
    "        indices = np.where(baseline_targets == target)[0]\n",
    "        bench_transform_values = baseline_bench_pca[indices]\n",
    "        ideal_transform_values = baseline_ideal_pca[indices]\n",
    "        sim_transform_values = baseline_sim_pca[indices]\n",
    "        color_idx = color2_indices[target]\n",
    "        color = colors2[color_idx]\n",
    "    \n",
    "        x_vals = ideal_transform_values[:,0]\n",
    "        y_vals = ideal_transform_values[:,1]\n",
    "        ax[0][0].scatter(x_vals, y_vals, color=color, label = target)\n",
    "        ax[0][0].set_title(\"Ideal image embeddings\")\n",
    "    \n",
    "        x_vals = sim_transform_values[:,0]\n",
    "        y_vals = sim_transform_values[:,1]\n",
    "        ax[0][1].scatter(x_vals, y_vals, color=color, label = target)\n",
    "        ax[0][1].set_title(\"Simulated image embeddings\")\n",
    "\n",
    "        x_vals = bench_transform_values[:,0]\n",
    "        y_vals = bench_transform_values[:,1]\n",
    "        ax[0][2].scatter(x_vals, y_vals, color=color, label = target)\n",
    "        ax[0][2].set_title(\"Bench image embeddings\")\n",
    "\n",
    "for target in pt_unique_targets:\n",
    "        indices = np.where(pt_targets == target)[0]\n",
    "        baseline_indices = np.where(baseline_targets == target)[0]\n",
    "        ideal_transform_values = baseline_ideal_pca[baseline_indices]\n",
    "\n",
    "        bench_transform_values = pt_bench_pca[indices]\n",
    "        sim_transform_values = pt_sim_pca[indices]\n",
    "        color_idx = color2_indices[target]\n",
    "        color = colors2[color_idx]\n",
    "    \n",
    "        x_vals = ideal_transform_values[:,0]\n",
    "        y_vals = ideal_transform_values[:,1]\n",
    "        ax[1][0].scatter(x_vals, y_vals, color=color, label = target)\n",
    "        ax[1][0].set_title(\"Ideal image embeddings\")\n",
    "    \n",
    "        x_vals = sim_transform_values[:,0]\n",
    "        y_vals = sim_transform_values[:,1]\n",
    "        ax[1][1].scatter(x_vals, y_vals, color=color, label = target)\n",
    "        ax[1][1].set_title(\"Simulated image embeddings\")\n",
    "\n",
    "        x_vals = bench_transform_values[:,0]\n",
    "        y_vals = bench_transform_values[:,1]\n",
    "        ax[1][2].scatter(x_vals, y_vals, color=color, label = target)\n",
    "        ax[1][2].set_title(\"Bench image embeddings\")\n",
    "\n",
    "\n",
    "\n",
    "for ax in ax.flatten():\n",
    "    ax.set_aspect('equal')\n",
    "    ax.legend(frameon=True, framealpha=1)\n",
    "    ax.set_xlim(-5.5, 5.5)\n",
    "    ax.set_ylim(-5.5, 5.5)\n",
    "plt.tight_layout()\n",
    "fig.savefig('feature_space_comparison.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ba57f6-fe55-4ae1-8000-557a3f7b103b",
   "metadata": {},
   "source": [
    "# UMAP comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a879aa24-b891-4d00-91ce-0c7f7e919994",
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_transform = umap.UMAP(n_neighbors=5, random_state=42).fit(baseline_ideal_feature_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba1185a-1561-45c6-923a-fe66184c2ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_ideal_umap = umap_transform.transform(baseline_ideal_feature_embeddings)\n",
    "baseline_bench_umap = umap_transform.transform(baseline_bench_feature_embeddings)\n",
    "baseline_sim_umap = umap_transform.transform(baseline_sim_feature_embeddings)\n",
    "pt_bench_umap = umap_transform.transform(pt_bench_feature_embeddings)\n",
    "pt_sim_umap = umap_transform.transform(pt_sim_feature_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66d4315-1681-4084-b3c2-539da363d2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(2,3, figsize=(15,10))\n",
    "\n",
    "for target in baseline_unique_targets:\n",
    "        indices = np.where(baseline_targets == target)[0]\n",
    "        bench_transform_values = baseline_bench_umap[indices]\n",
    "        ideal_transform_values = baseline_ideal_umap[indices]\n",
    "        sim_transform_values = baseline_sim_umap[indices]\n",
    "        color_idx = color2_indices[target]\n",
    "        color = colors2[color_idx]\n",
    "    \n",
    "        x_vals = ideal_transform_values[:,0]\n",
    "        y_vals = ideal_transform_values[:,1]\n",
    "        ax[0][0].scatter(x_vals, y_vals, color=color, label = target)\n",
    "        ax[0][0].set_title(\"Ideal image embeddings\")\n",
    "    \n",
    "        x_vals = sim_transform_values[:,0]\n",
    "        y_vals = sim_transform_values[:,1]\n",
    "        ax[0][1].scatter(x_vals, y_vals, color=color, label = target)\n",
    "        ax[0][1].set_title(\"Simulated image embeddings\")\n",
    "\n",
    "        x_vals = bench_transform_values[:,0]\n",
    "        y_vals = bench_transform_values[:,1]\n",
    "        ax[0][2].scatter(x_vals, y_vals, color=color, label = target)\n",
    "        ax[0][2].set_title(\"Bench image embeddings\")\n",
    "\n",
    "for target in pt_unique_targets:\n",
    "        indices = np.where(pt_targets == target)[0]\n",
    "        baseline_indices = np.where(baseline_targets == target)[0]\n",
    "        ideal_transform_values = baseline_ideal_umap[baseline_indices]\n",
    "\n",
    "        bench_transform_values = pt_bench_umap[indices]\n",
    "        sim_transform_values = pt_sim_umap[indices]\n",
    "        color_idx = color2_indices[target]\n",
    "        color = colors2[color_idx]\n",
    "    \n",
    "        x_vals = ideal_transform_values[:,0]\n",
    "        y_vals = ideal_transform_values[:,1]\n",
    "        ax[1][0].scatter(x_vals, y_vals, color=color, label = target)\n",
    "        ax[1][0].set_title(\"Ideal image embeddings\")\n",
    "    \n",
    "        x_vals = sim_transform_values[:,0]\n",
    "        y_vals = sim_transform_values[:,1]\n",
    "        ax[1][1].scatter(x_vals, y_vals, color=color, label = target)\n",
    "        ax[1][1].set_title(\"Simulated image embeddings\")\n",
    "\n",
    "        x_vals = bench_transform_values[:,0]\n",
    "        y_vals = bench_transform_values[:,1]\n",
    "        ax[1][2].scatter(x_vals, y_vals, color=color, label = target)\n",
    "        ax[1][2].set_title(\"Bench image embeddings\")\n",
    "\n",
    "\n",
    "\n",
    "for ax in ax.flatten():\n",
    "    ax.set_aspect('equal')\n",
    "    ax.legend(frameon=True, framealpha=1)\n",
    "    ax.set_xlim(-10, 20)\n",
    "    ax.set_ylim(-10, 20)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d37c3bf-2ebd-4ff1-8b9b-a684bad12b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_transform = umap.UMAP(n_neighbors=5, random_state=42).fit(pt_bench_feature_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c813ba2-0a50-4236-8213-085c88b9b53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_bench_umap = umap_transform.transform(pt_bench_feature_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffc64c9-22e4-4fdd-8b67-583e1a39afb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1, figsize=(5,5))\n",
    "\n",
    "for target in pt_unique_targets:\n",
    "        indices = np.where(pt_targets == target)[0]\n",
    "        bench_transform_values = pt_bench_umap[indices]\n",
    "        color_idx = color2_indices[target]\n",
    "        color = colors2[color_idx]\n",
    "    \n",
    "        x_vals = ideal_transform_values[:,0]\n",
    "        y_vals = ideal_transform_values[:,1]\n",
    "        ax.scatter(x_vals, y_vals, color=color, label = target)\n",
    "        ax.set_title(\"Bench image embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832fb98f-69d7-4674-9c09-ed09981785c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
