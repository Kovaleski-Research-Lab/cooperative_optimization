{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3b7cfb-448d-4d64-bffc-1ccc4c6fe5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import yaml\n",
    "import sys\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "sys.path.append('../')\n",
    "from datamodule.datamodule import select_data\n",
    "from models.models import Classifier, CooperativeOpticalModelRemote\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "from sklearn import datasets, decomposition\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668048ce-e0b7-4985-8483-c6381056cb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(plt.style.available)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25aff4fd-6cc6-4ae7-8ecf-c8d9614566a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = yaml.load(open('../../config.yaml', 'r'), Loader=yaml.FullLoader)\n",
    "config['paths']['path_root'] = '../../'\n",
    "config['paths']['path_data'] = 'data/baseline'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64f842f-d659-4f99-848e-b360dea103bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [os.path.join(config['paths']['path_root'], config['paths']['path_data'], i) for i in os.listdir(os.path.join(config['paths']['path_root'], config['paths']['path_data']))]\n",
    "filenames.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b52e6d-d7b9-407d-8ad5-969d74c27cc2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bench_images = [torch.load(f, weights_only=True)['bench_image'].squeeze().numpy() for f in tqdm(filenames)]\n",
    "sim_images = [torch.load(f, weights_only=True)['sim_output'].squeeze().numpy() for f in tqdm(filenames)]\n",
    "ideal_images = [torch.load(f, weights_only=True)['resampled_sample'].squeeze().numpy() for f in tqdm(filenames)]\n",
    "\n",
    "targets = [torch.argmax(torch.load(f, weights_only=True)['target']).numpy() for f in tqdm(filenames)]\n",
    "targets = np.asarray(targets).squeeze()\n",
    "unique_targets = np.unique(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df657b33-a9f0-45da-8dcd-d9f9f8cc936f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_vectors(classifier, images):\n",
    "    feature_vectors = []\n",
    "    for image in tqdm(images):\n",
    "        image = torch.from_numpy(image).squeeze().unsqueeze(0).unsqueeze(0)\n",
    "        image = torch.cat([image, image, image], dim=1).double()\n",
    "        feature_vectors.append(classifier.feature_extractor(image))\n",
    "    return feature_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7742ec0c-3acc-4ab1-8e35-6967b668e252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in a pretrained classifier\n",
    "# For every image in a dataset, get the feature embedding\n",
    "checkpoint_path = '../../results/classifier_baseline_bench_resampled_sample/version_0/checkpoints/last.ckpt'\n",
    "classifier = Classifier.load_from_checkpoint(checkpoint_path).double().cpu()\n",
    "\n",
    "with torch.no_grad():\n",
    "    bench_image_feature_embeddings = create_feature_vectors(classifier, bench_images)\n",
    "    sim_image_feature_embeddings = create_feature_vectors(classifier, sim_images)\n",
    "    ideal_image_feature_embeddings = create_feature_vectors(classifier, ideal_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1210818-ce80-4bf0-9363-96ab57dad802",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_bench_image_feature_embeddings = np.asarray([i.squeeze() for i in bench_image_feature_embeddings])\n",
    "np_sim_image_feature_embeddings = np.asarray([i.squeeze() for i in sim_image_feature_embeddings])\n",
    "np_ideal_image_feature_embeddings = np.asarray([i.squeeze() for i in ideal_image_feature_embeddings])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bfe6f4-b938-4091-91e9-b97cfc094c92",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d235718-b024-455b-996f-37269656465f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_pca_results = '../../results/feature_space_analysis/pca'\n",
    "os.makedirs(path_pca_results, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3d27d3-1584-4daf-94ab-b3473ef72a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_components = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a124b7cb-1e4e-4b84-bd0f-6216397ee1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['#E8ECFB', '#D9CCE3', '#D1BBD7', '#CAACCB', '#BA8DB4', \n",
    "          '#AE76A3', '#AA6F9E', '#994F88', '#882E72', '#1965B0', \n",
    "          '#437DBF', '#5289C7', '#6195CF', '#7BAFDE', '#4EB265', \n",
    "          '#90C987', '#CAE0AB', '#F7F056', '#F7CB45', '#F6C141', \n",
    "          '#F4A736', '#F1932D', '#EE8026', '#E8601C', '#E65518', \n",
    "          '#DC050C', '#A5170E', '#72190E', '#42150A']\n",
    "\n",
    "colors2 = [ '#a6cee3',\n",
    "            '#1f78b4',\n",
    "            '#b2df8a',\n",
    "            '#33a02c',\n",
    "            '#fb9a99',\n",
    "            '#e31a1c',\n",
    "            '#fdbf6f',\n",
    "            '#ff7f00',\n",
    "            '#cab2d6',\n",
    "            '#6a3d9a']\n",
    "\n",
    "color_indices = [9,10,14,15,17,18,21,24,26,28]\n",
    "color2_indices = [0,1,2,3,4,5,6,7,8,9]\n",
    "len(color_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bc5ce5-6858-4329-882d-5563aaef829c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "pca.fit(np_ideal_image_feature_embeddings)\n",
    "bench_transform = pca.transform(np_bench_image_feature_embeddings)\n",
    "ideal_transform = pca.transform(np_ideal_image_feature_embeddings)\n",
    "sim_transform = pca.transform(np_sim_image_feature_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45494c4d-4454-4fdd-bd23-96fea64009a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,3, figsize=(15,5))\n",
    "\n",
    "for target in unique_targets:\n",
    "        indices = np.where(targets == target)[0]\n",
    "        bench_transform_values = bench_transform[indices]\n",
    "        ideal_transform_values = ideal_transform[indices]\n",
    "        sim_transform_values = sim_transform[indices]\n",
    "        color_idx = color2_indices[target]\n",
    "        color = colors2[color_idx]\n",
    "        x_vals = ideal_transform_values[:,0]\n",
    "        y_vals = ideal_transform_values[:,1]\n",
    "        ax[0].scatter(x_vals, y_vals, color=color, label = target)\n",
    "        ax[0].set_title(\"Ideal image embeddings\")\n",
    "        x_vals = sim_transform_values[:,0]\n",
    "        y_vals = sim_transform_values[:,1]\n",
    "        ax[1].scatter(x_vals, y_vals, color=color, label = target)\n",
    "        ax[1].set_title(\"Simulated image embeddings\")\n",
    "\n",
    "        x_vals = bench_transform_values[:,0]\n",
    "        y_vals = bench_transform_values[:,1]\n",
    "        ax[2].scatter(x_vals, y_vals, color=color, label = target)\n",
    "        ax[2].set_title(\"Bench image embeddings\")\n",
    "\n",
    "\n",
    "for ax in ax.flatten():\n",
    "    ax.set_aspect('equal')\n",
    "    ax.legend(frameon=True, framealpha=1)\n",
    "    ax.set_xlim(-5.5, 5.5)\n",
    "    ax.set_ylim(-5.5, 5.5)\n",
    "plt.tight_layout()\n",
    "fig.savefig(os.path.join(path_pca_results, 'pca_embeddings.pdf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8f99fe-e8e1-42b7-8aeb-1d73dba8ed7e",
   "metadata": {},
   "source": [
    "# TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6eb59c-20b1-4ec0-9f60-90a141f3bffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_tsne_results = '../../results/feature_space_analysis/tsne'\n",
    "os.makedirs(path_tsne_results, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4aa3b7-7863-4458-89f9-a1288ebdb3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components = 2, learning_rate='auto', init='random', perplexity=50).fit(np_ideal_image_feature_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c93543-440d-4c7f-b28d-dfb087fec82a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "unique_targets = np.unique(targets)\n",
    "for j in tqdm(range(5,50)):\n",
    "    plt.close('all')\n",
    "    tsne = TSNE(n_components = 2, learning_rate='auto', init='random', perplexity=j).fit(np_ideal_image_feature_embeddings)\n",
    "    fig,ax = plt.subplots(1,1,figsize=(5,5))\n",
    "\n",
    "    for target in unique_targets:\n",
    "        indices = np.where(targets == target)[0]\n",
    "        tsne_vals = tsne[indices]\n",
    "        x_vals = tsne_vals[:,0]\n",
    "        y_vals = tsne_vals[:,1]\n",
    "        color_idx = color_indices[target]\n",
    "        color = colors[color_idx]\n",
    "        ax.scatter(x_vals, y_vals, color=color, label = target)\n",
    "    ax.legend()\n",
    "    ax.set_title(f\"Ideal - Ideal Perplexity = {j:03d}\")\n",
    "    fig.savefig(os.path.join(path_tsne_results, f'ideal_ideal_perplexity_{j:03d}.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fa3907-ab75-4664-bffc-b897fb17cc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.join(path_tsne_results, f\"ideal_ideal_perplexity_{j}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444f7c29-878c-4372-8553-8b44ca0ea15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = np.asarray(targets)\n",
    "print(targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0aef994-d8a5-4f78-8a17-9091d8af1a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(targets==1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc62a85a-4dfd-484d-9950-b1f65a687acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ad9395-655a-4310-b204-1d3141d4562d",
   "metadata": {},
   "source": [
    "# UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29b4b6c-fbcf-4a80-abb8-2731dd5c9597",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_umap_results = '../../results/feature_space_analysis/umap'\n",
    "os.makedirs(path_umap_results, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26593ad3-1050-4805-ad9e-83dda1b03558",
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_transform = umap.UMAP(n_neighbors=5, random_state=42).fit(np_ideal_image_feature_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e72989-0cce-4093-a9e1-9b621b0b0b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "ideal_umap = umap_transform.transform(np_ideal_image_feature_embeddings)\n",
    "bench_umap = umap_transform.transform(np_bench_image_feature_embeddings)\n",
    "sim_umap = umap_transform.transform(np_sim_image_feature_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd269221-372f-40a5-84e4-e3f037ee853e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,3, figsize=(15,5))\n",
    "\n",
    "for target in unique_targets:\n",
    "        indices = np.where(targets == target)[0]\n",
    "        bench_transform_values = bench_umap[indices]\n",
    "        ideal_transform_values = ideal_umap[indices]\n",
    "        sim_transform_values = sim_umap[indices]\n",
    "        color_idx = color2_indices[target]\n",
    "        color = colors2[color_idx]\n",
    "        x_vals = ideal_transform_values[:,0]\n",
    "        y_vals = ideal_transform_values[:,1]\n",
    "        ax[0].scatter(x_vals, y_vals, color=color, label = target, alpha=1)\n",
    "        ax[0].set_title(\"Ideal image embeddings\")\n",
    "        x_vals = sim_transform_values[:,0]\n",
    "        y_vals = sim_transform_values[:,1]\n",
    "        ax[1].scatter(x_vals, y_vals, color=color, label = target, alpha=1)\n",
    "        ax[1].set_title(\"Simulated image embeddings\")\n",
    "        x_vals = bench_transform_values[:,0]\n",
    "        y_vals = bench_transform_values[:,1]\n",
    "        ax[2].scatter(x_vals, y_vals, color=color, label = target, alpha=1)\n",
    "        ax[2].set_title(\"Bench image embeddings\")\n",
    "\n",
    "for ax in ax.flatten():\n",
    "    ax.set_aspect('equal')\n",
    "    ax.legend(frameon=True, framealpha=0.5)\n",
    "    ax.set_xlim(-10, 20)\n",
    "    ax.set_ylim(-10, 20)\n",
    "plt.tight_layout()\n",
    "fig.savefig(os.path.join(path_umap_results, 'umap_feature_embeddings.pdf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80b3b45-e0f9-42f4-a398-134d5536bd54",
   "metadata": {},
   "source": [
    "# Post training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a45560d-67cf-477c-b12b-04a5d62b064e",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = yaml.load(open('../../results/coop_bench_alpha_0.0_beta_0.0_gamma_0.0_delta_1.0/version_2/config.yaml', 'r'), Loader=yaml.FullLoader)\n",
    "\n",
    "config['paths']['path_data'] = 'data/post_training'\n",
    "path_pca_results_pt = '../../results/path_pca_results_pt'\n",
    "os.makedirs(path_pca_results_pt, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06ed145-4953-4f9d-8047-d3040d9ad609",
   "metadata": {},
   "outputs": [],
   "source": [
    "config['classifier']['checkpoint_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969e0752-bee3-4cf2-8099-e85f2a5ed558",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [os.path.join(config['paths']['path_root'], config['paths']['path_data'], i) for i in os.listdir(os.path.join(config['paths']['path_root'], config['paths']['path_data']))]\n",
    "filenames.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2821fcaa-a285-4e55-8a3a-670e173289ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_bench_images = [torch.load(f, weights_only=True)['bench_image'].squeeze().detach().numpy() for f in tqdm(filenames)]\n",
    "pt_sim_images = [torch.load(f, weights_only=True)['sim_output'].squeeze().detach().numpy() for f in tqdm(filenames)]\n",
    "pt_ideal_images = [torch.load(f, weights_only=True)['resampled_sample'].squeeze().detach().numpy() for f in tqdm(filenames)]\n",
    "\n",
    "pt_targets = [torch.argmax(torch.load(f, weights_only=True)['target']).numpy() for f in tqdm(filenames)]\n",
    "pt_targets = np.asarray(targets).squeeze()\n",
    "pt_unique_targets = np.unique(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb3155f-4784-4649-b0cb-0297f9f018fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in a pretrained classifier\n",
    "# For every image in a dataset, get the feature embedding\n",
    "checkpoint_path = '../../results/classifier_baseline_bench_resampled_sample/version_0/checkpoints/last.ckpt'\n",
    "classifier = Classifier.load_from_checkpoint(checkpoint_path).double().cpu()\n",
    "\n",
    "with torch.no_grad():\n",
    "    pt_bench_image_feature_embeddings = create_feature_vectors(classifier, bench_images)\n",
    "    pt_sim_image_feature_embeddings = create_feature_vectors(classifier, sim_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c66e31-c997-4291-8724-7c5053b57f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_np_bench_image_feature_embeddings = np.asarray([i.squeeze() for i in pt_bench_image_feature_embeddings])\n",
    "pt_np_sim_image_feature_embeddings = np.asarray([i.squeeze() for i in pt_sim_image_feature_embeddings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ef831c-cbfa-4266-8e53-d93190814947",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_bench_transform = pca.transform(pt_np_bench_image_feature_embeddings)\n",
    "pt_sim_transform = pca.transform(pt_np_sim_image_feature_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c02a14e-74dc-4510-a8f3-f2cc09253503",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,3, figsize=(15,5))\n",
    "\n",
    "for target in unique_targets:\n",
    "        pt_indices = np.where(pt_targets == target)[0]\n",
    "        indices = np.where(targets == target)[0]\n",
    "        bench_transform_values = pt_bench_transform[pt_indices]\n",
    "        ideal_transform_values = ideal_transform[indices]\n",
    "        sim_transform_values = pt_sim_transform[pt_indices]\n",
    "        color_idx = color2_indices[target]\n",
    "        color = colors2[color_idx]\n",
    "        x_vals = ideal_transform_values[:,0]\n",
    "        y_vals = ideal_transform_values[:,1]\n",
    "        ax[0].scatter(x_vals, y_vals, color=color, label = target)\n",
    "        ax[0].set_title(\"Ideal image embeddings\")\n",
    "        x_vals = sim_transform_values[:,0]\n",
    "        y_vals = sim_transform_values[:,1]\n",
    "        ax[1].scatter(x_vals, y_vals, color=color, label = target)\n",
    "        ax[1].set_title(\"Simulated image embeddings\")\n",
    "\n",
    "        x_vals = bench_transform_values[:,0]\n",
    "        y_vals = bench_transform_values[:,1]\n",
    "        ax[2].scatter(x_vals, y_vals, color=color, label = target)\n",
    "        ax[2].set_title(\"Bench image embeddings\")\n",
    "\n",
    "\n",
    "for ax in ax.flatten():\n",
    "    ax.set_aspect('equal')\n",
    "    ax.legend(frameon=True, framealpha=1)\n",
    "    ax.set_xlim(-5.5, 5.5)\n",
    "    ax.set_ylim(-5.5, 5.5)\n",
    "plt.tight_layout()\n",
    "fig.savefig(os.path.join(path_pca_results_pt, 'pca_embeddings.pdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f898d6bd-09f4-4365-a430-10d3650ff6f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
